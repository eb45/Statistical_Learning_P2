---
title: "Project 2: Heart Disease Dataset"
author: "Emma Bennett, Pallavi Bhargava, Lily Berner"
date: 'UC3M, 2025'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 6, fig.height = 4)
library(tidyverse)
library(caret)
library(VIM)
library(mice)
library(randomForest)
library(pROC)
library(MASS)
library(ggplot2)
library(rpart)
library(rpart.plot)
```

### Introduction

Predict presence of heart disease (num) from clinical variables in heart_disease_uci.csv.

### Loading data

```{r}
# Loading CSV
heart_dis <- read.csv("heart_disease_uci.csv", stringsAsFactors = FALSE)

# Remove unwanted columns
heart_dis <- heart_dis[ , -c(1,4)]

# Getting summary
str(heart_dis)
summary(heart_dis)
```

### Missing values and imputation

```{r}
# Look what values are missing
VIM::aggr(heart_dis, numbers = TRUE, sortVars = TRUE, labels = names(heart_dis),
cex.axis = 0.7, gap = 1, ylab = c("Missing data","Pattern"))
```

Use the mice library to inspect possible imputations

```{r}
mice::md.pattern(heart_dis)

# Median imputation for selected numeric columns
cols_to_impute <- c("ca", "fbs", "oldpeak", "trestbps", "chol", "thalach", "exang")
cols_to_impute <- intersect(cols_to_impute, names(heart_dis))

for (col in cols_to_impute) {
  
  if (is.numeric(heart_dis[[col]])) {
    
    # direct median imputation
    heart_dis[[col]][is.na(heart_dis[[col]])] <- median(heart_dis[[col]], na.rm = TRUE)
    
  } else {
    
    # convert to numeric if needed
    heart_dis[[col]] <- type.convert(heart_dis[[col]], as.is = TRUE)
    
    # median imputation after conversion
    heart_dis[[col]][is.na(heart_dis[[col]])] <- median(as.numeric(heart_dis[[col]]), na.rm = TRUE)
  }
}
```

Verify no null values remain:

```{r}
sapply(heart_dis, function(x) sum(is.na(x)))
summary(heart_dis)
```

### Feature Engineering

Convert to binary presence (0 = no disease, 1 = disease)

```{r}
if("num" %in% names(heart_dis)){
  heart_dis$num <- ifelse(as.numeric(heart_dis$num) > 0, 1, 0)
  heart_dis$num <- factor(heart_dis$num, levels = c(0,1), labels = c("No","Yes"))
}
```

Convert obviously categorical numeric columns to factors

```{r}
cat_vars <- c("sex","cp","fbs","restecg","exang","slope","ca","thal")
cat_vars <- intersect(cat_vars, names(heart_dis))
for(v in cat_vars) heart_dis[[v]] <- factor(heart_dis[[v]])

str(heart_dis)
```

### Train/Test Split

```{r}
set.seed(2025)
idx <- createDataPartition(heart_dis$num, p = 0.8, list = FALSE)

training <- heart_dis[idx, ]
testing  <- heart_dis[-idx, ]

nrow(training); nrow(testing)
table(training$num)
prop.table(table(training$num))
```

### Exploratory Plots/Visualization

```{r}
# Density of cholesterol by disease presence
if("chol" %in% names(training)){
  ggplot(training, aes(x = chol, fill = num, colour = num)) +
    geom_density(alpha = 0.2) +
    labs(title = "Cholesterol distribution by heart disease", x = "chol")
}

# Age vs max heart rate colored by outcome
if(all(c("age","thalach") %in% names(training))){
  ggplot(training, aes(x = age, y = thalach, colour = num)) +
  geom_point(alpha = 0.7) +
  labs(title = "Age vs Max Heart Rate", x = "age", y = "thalach")
}

# Chest pain type vs disease proportion
if("cp" %in% names(training)){
ggplot(training, aes(x = cp, fill = num)) + geom_bar(position = "fill") +
labs(y = "Proportion", title = "Chest pain type vs disease proportion")
}
```

# Baseline classification model

Computing majority-class baseline

```{r}
# Class proportions in training set
cat("Training class distribution:\n")
print(prop.table(table(training$num)))

# Majority class prediction
majority_class <- names(sort(table(training$num), decreasing = TRUE))[1]
cat("\nMajority class (baseline):", majority_class, "\n")

# Baseline predictions on testset
baseline_pred <- factor(rep(majority_class, nrow(testing)), levels = levels(training$num))

#Baseline performance
baseline_cm <- confusionMatrix(baseline_pred, testing$num)
cat("\nBaseline confusion matrix:\n")
print(baseline_cm$table)
cat("\nBaseline metrics:\n")
print(baseline_cm$overall)
```

## Modeling

### Logistic Regression
Below I'm going to fit a logistic regression on the training set, eval it on the testset, inspect the ROC/AUC, choose an optimal threshold, and then interpret the main coefficients.

Going to use glm function like the Credit Scoring Case study we did.

```{r}
logit.model <- glm(num ~ ., data = training, family = binomial(link = "logit"))
summary(logit.model)
```

```{r}
# Predicted probabilities on test set
glm_prob <- predict(logit.model, newdata = testing, type = "response")
head(glm_prob)
```

```{r}
# default threshold = 0.5
glm_pred_05 <- factor(ifelse(glm_prob > 0.5, "Yes", "No"), levels = levels(training$num))
glm_cm_05 <- confusionMatrix(glm_pred_05, testing$num, positive = "Yes")
print(glm_cm_05)
```

```{r}
# ROC and AUC
library(pROC)
roc_glm <- roc(response = testing$num, predictor = glm_prob, levels = c("No", "Yes"))
auc_glm <- as.numeric(auc(roc_glm))
cat("Logistic AUC =", round(auc_glm, 3), "\n")
plot.roc(roc_glm, col = "darkblue", print.auc = TRUE, legacy.axes = TRUE,
main = paste0("Logistic ROC (AUC=", round(auc_glm,3), ")"))
```

```{r}
# Optimal threshold and eval at that threshold
best <- coords(roc_glm, "best", best.method = "youden", ret = c("threshold","sensitivity","specificity","accuracy"), transpose = FALSE)
best
best_thr <- as.numeric(best["threshold"])
cat("Youden threshold =", round(best_thr,3), "\n")

glm_pred_best <- factor(ifelse(glm_prob > best_thr, "Yes", "No"), levels = levels(training$num))
glm_cm_best <- confusionMatrix(glm_pred_best, testing$num, positive = "Yes")
print(glm_cm_best)
```

```{r}
### Threshold trade-off plot

# 1. Create sequence of thresholds
thrs <- seq(0.01, 0.99, by = 0.01)

# 2. Compute sensitivity, specificity, accuracy at each threshold
metrics_list <- lapply(thrs, function(t) {
  
  pred <- factor(ifelse(glm_prob > t, "Yes", "No"),
                 levels = levels(training$num))
  
  cm <- confusionMatrix(pred, testing$num, positive = "Yes")
  
  data.frame(
    threshold   = t,
    sensitivity = as.numeric(cm$byClass["Sensitivity"]),
    specificity = as.numeric(cm$byClass["Specificity"]),
    accuracy    = as.numeric(cm$overall["Accuracy"])
  )
})

# 3. Combine all rows into one dataframe
metrics_df <- do.call(rbind, metrics_list)

# 4. Reshape for ggplot (long format)
library(tidyr)
metrics_long <- pivot_longer(
  metrics_df,
  cols = c(sensitivity, specificity, accuracy),
  names_to = "metric",
  values_to = "value"
)

# 5. Plot
library(ggplot2)
ggplot(metrics_long, aes(x = threshold, y = value, colour = metric)) +
  geom_line(size = 0.9) +
  geom_vline(xintercept = best_thr, linetype = "dashed") +
  labs(
    title = "Threshold trade-offs (Logistic Regression)",
    x = "Threshold",
    y = "Metric Value"
  ) +
  scale_colour_manual(values = c(
    "sensitivity" = "blue",
    "specificity" = "red",
    "accuracy" = "darkgreen"
  )) +
  theme_minimal()


```

### Decision Tree
Referenced the Customer Churn case study.

```{r}
# minsplit: minimum number of observations in a node before before a split
# maxdepth: maximum depth of any node of the final tree
# cp: degree of complexity, the smaller the more branches
control_shallow <- rpart.control(minsplit = 30, maxdepth = 3, cp = 0.001)
```

```{r}
# Fit shallow decision tree
set.seed(2025)
dt_shallow <- rpart(num ~ ., data = training, method = "class", control = control_shallow)

print(dt_shallow)
summary(dt_shallow)

# Plot shallow tree
rpart.plot(dt_shallow,
type = 2,
extra = 104,
fallen.leaves = TRUE,
box.palette = "GnBu",
shadow.col = "gray",
nn = TRUE, # show node numbers (useful for rules)
tweak = 1.1, 
cex = 0.9, # font size
main = "Pruned Decision Tree")
```


```{r}
# Predictions
dt_prob <- predict(dt_shallow, newdata = testing, type = "prob")[, "Yes"]
dt_class_05 <- factor(ifelse(dt_prob > 0.5, "Yes", "No"), levels = levels(training$num))
dt_cm_05 <- confusionMatrix(dt_class_05, testing$num, positive = "Yes")
dt_cm_05
```


### Random Forest
```{r}
# Make sure that there are no missing values
sapply(training, function(x) sum(is.na(x)))
sapply(testing,  function(x) sum(is.na(x)))

which_row_nas_train <- which(rowSums(is.na(training))>0)
length(which_row_nas_train)  # how many rows in training with any NA
which_row_nas_test  <- which(rowSums(is.na(testing))>0)
length(which_row_nas_test)

impute_mode <- function(x) {
ux <- unique(na.omit(x))
ux[which.max(tabulate(match(x, ux)))]
}

impute_df <- function(df) {
for(col in names(df)) {
if(is.numeric(df[[col]])) {
if(any(is.na(df[[col]]))) df[[col]][is.na(df[[col]])] <- median(df[[col]], na.rm = TRUE)
} else {

  if(is.character(df[[col]])) df[[col]][df[[col]]==""] <- NA
if(any(is.na(df[[col]]))) {
m <- impute_mode(df[[col]])
df[[col]][is.na(df[[col]])] <- m
}
# ensure factors remain factors
if(!is.factor(df[[col]])) df[[col]] <- factor(df[[col]])
}
}
df
}

training <- impute_df(training)
testing  <- impute_df(testing)

sapply(training, function(x) sum(is.na(x)))
sapply(testing,  function(x) sum(is.na(x)))

# Ensure factor levels align between training and testing (important for predict)

factor_cols <- names(training)[sapply(training, is.factor)]
for(col in factor_cols) {

# Unify levels: union of levels from both sets

levs <- union(levels(training[[col]]), levels(testing[[col]]))
training[[col]] <- factor(training[[col]], levels = levs)
testing[[col]]  <- factor(testing[[col]],  levels = levs)
}

# Now re-run Random Forest (randomForest) - original format
library(randomForest)
library(caret)
set.seed(2025)

rf.train <- randomForest(num ~ .,
data = training,
ntree = 500,
mtry = floor(sqrt(ncol(training) - 1)),
importance = TRUE)

# Predictions
rf.pred <- predict(rf.train, newdata = testing)
confusionMatrix(rf.pred, testing$num)

# If you want probabilities/AUC

rf.prob <- predict(rf.train, newdata = testing, type = "prob")[, "Yes"]
library(pROC)
roc_rf <- roc(response = testing$num, predictor = rf.prob, levels = c("No","Yes"))
auc(roc_rf)
```


### Model tuned with caret

```{r}
ctrl_roc <- trainControl(
method = "repeatedcv",
number = 5,
repeats = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final",
verboseIter = TRUE
)

num_features <- ncol(training) - 1
tune_grid <- expand.grid(
mtry = unique(pmax(1, c(1, floor(sqrt(num_features)), floor(num_features/3)))),
splitrule = "gini",
min.node.size = c(1, 3, 5)
)

set.seed(2025)
rf_tuned <- train(
num ~ .,
data = training,
method = "ranger",
metric = "ROC",
tuneGrid = tune_grid,
trControl = ctrl_roc,
num.trees = 1000,
importance = "impurity"
)

rf_tuned
rf_tuned$bestTune


best_row <- rf_tuned$results[
which(rf_tuned$results$mtry == rf_tuned$bestTune$mtry &
rf_tuned$results$min.node.size == rf_tuned$bestTune$min.node.size), ]
best_row[, c("ROC", "ROCSD")]

rf_tuned_prob <- predict(rf_tuned, newdata = testing, type = "prob")[, "Yes"]
roc_rf_tuned <- roc(response = testing$num, predictor = rf_tuned_prob, levels = c("No","Yes"))
auc_test <- as.numeric(auc(roc_rf_tuned))
auc_test

plot.roc(roc_rf_tuned, print.auc = TRUE)

```

### Add SGboost

## Interpretability

## Using function varimp to plot variable importance

# Feature selection take most important variables

## Partial dependence plot
